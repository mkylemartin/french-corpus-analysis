{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141713828\n"
     ]
    }
   ],
   "source": [
    "corpus_path = ('/Users/Kyle/Documents/Box Sync/old/'\n",
    "               'MARTIN-french-corpus-github/'\n",
    "               '5_french-news-corpus.txt')\n",
    "RAW_CORPUS = open(corpus_path, 'r').read()\n",
    "print(len(RAW_CORPUS)) # how much did I start with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove non-interesting information\n",
    "\n",
    "For better n-grams!\n",
    "\n",
    "1. Remove a `cookie_message` on all of the sites that sort of messes with our ability to pull meaningful n-grams out of the corpus.\n",
    "\n",
    "2. Remove a slogan (`lemonde_slogan`) that's at the bottom of all articles by [lemonde.fr](https://lemonde.fr)\n",
    "\n",
    "3. Related info message (`lemonde_info`) also removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "undesirables = {}\n",
    "undesirables['cookie_message'] = (\"En poursuivant votre navigation sur ce site, \"\n",
    "                                  \"vous acceptez nos CGV et l\\’utilisation de cookies \"\n",
    "                                  \"pour vous proposer des contenus et services adaptés\"\n",
    "                                  \" à vos centres d’intérêts et vous permettre \"\n",
    "                                  \"l\\'utilisation de boutons de partages sociaux.\"\n",
    "                                  r\"\\s+En savoir plus et gérer ces paramètres\\.\")\n",
    "\n",
    "undesirables['lemonde_slogan'] = (r\"Découvrez chaque jour toute l\\'info en\\s+\"\n",
    "                                  r\"direct \\(de la politique à l\\'économie en \"\n",
    "                                  r\"passant par le sport et la\\s+\"\n",
    "                                  r\"météo\\) sur Le Monde\\.fr, le site de news \"\n",
    "                                  r\"leader de la presse française\\s+\"\n",
    "                                  r\"en ligne\\.\")\n",
    "\n",
    "undesirables['lemonde_info'] = (\"Journal d\\'information en ligne, \"\n",
    "                                \"Le Monde.fr offre à ses visiteurs \"\n",
    "                                r\"un\\s+panorama complet de l\\'actualité.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_undesirables(message, name, corpus):\n",
    "    \"\"\" receives a dictionary organized by \n",
    "        phenom and message to be removed. \"\"\"\n",
    "    amount = len(re.findall(message, corpus, flags=re.IGNORECASE))\n",
    "    filtered_corpus = re.sub(message, # to be subbed\n",
    "                             \"\", # remove this\n",
    "                             corpus, # current state of corpus\n",
    "                             flags=re.IGNORECASE) # just to be sure\n",
    "    print(f'removed {amount} instances of {name} `{message[:10]}(...)`')\n",
    "    \n",
    "    return filtered_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7142"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(re.findall(undesirables['lemonde_info'], RAW_CORPUS, flags=re.IGNORECASE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus filtration pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtration_pipeline(RAW_CORPUS):\n",
    "    \"\"\" filters the corpus removing the boogers for a breath of fresh air \"\"\"\n",
    "    \n",
    "    no_cookies = remove_undesirables(undesirables['cookie_message'],\n",
    "                                     'cookie_message', \n",
    "                                     RAW_CORPUS)\n",
    "\n",
    "    no_lemonde = remove_undesirables(undesirables['lemonde_slogan'],\n",
    "                                     'lemonde_slogan',\n",
    "                                     no_cookies)\n",
    "\n",
    "    no_info = remove_undesirables(undesirables['lemonde_info'],\n",
    "                                  'lemonde_info',\n",
    "                                  no_lemonde)\n",
    "\n",
    "    filtered = no_info\n",
    "\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed 7156 instances of cookie_message `En poursui(...)`\n",
      "removed 7142 instances of lemonde_slogan `Découvrez (...)`\n",
      "removed 7142 instances of lemonde_info `Journal d'(...)`\n"
     ]
    }
   ],
   "source": [
    "raw_filtered = filtration_pipeline(RAW_CORPUS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-baptism n-grams\n",
    "\n",
    "To complete this aspect of the project I will use the NLTK n-gram feature. I will first tokenize all of the text. Then I will pickle the tokens so that I can avoid reprocessing the mass of text each time I run the script. There is a simple implementation of n- grams I used in a previous assignment, and will be able to just reduplicate that code for this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `nltk.word_tokenize` everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_tokens = word_tokenize(raw_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = list(Counter(ngrams(nltk_tokens, 2)).items())\n",
    "trigrams = list(Counter(ngrams(nltk_tokens, 3)).items())\n",
    "fourgrams = list(Counter(ngrams(nltk_tokens, 4)).items())\n",
    "fivegrams = list(Counter(ngrams(nltk_tokens, 5)).items())\n",
    "\n",
    "grams = [bigrams, trigrams, fourgrams, fivegrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-grams\n",
      "4304622 grams\n",
      "3-grams\n",
      "12909766 grams\n",
      "4-grams\n",
      "19879549 grams\n",
      "5-grams\n",
      "23048889 grams\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "for gram in grams:\n",
    "    print(f'{i}-grams')\n",
    "    print(len(gram), 'grams')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Results  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19879549"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(fourgrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('•', 'Mis', 'à', 'jour', 'le'), 5063),\n",
       " (('--', '--', '--', '--', '--'), 2971),\n",
       " (('ce', 'n', '’', 'est', 'pas'), 1239),\n",
       " (('!', '!', '!', '!', '!'), 1143),\n",
       " (('il', 'n', '’', 'y', 'a'), 1134),\n",
       " (('En', 'poursuivant', 'votre', 'navigation', 'sur'), 1133),\n",
       " (('qu', '’', 'il', 'n', '’'), 1042),\n",
       " ((\"d'un\", 'droit', \"d'accès\", ',', 'de'), 1040),\n",
       " (('disposez', \"d'un\", 'droit', \"d'accès\", ','), 1026),\n",
       " (('droit', \"d'accès\", ',', 'de', 'rectification'), 1022),\n",
       " (('.', 'Conformément', 'à', 'la', 'loi'), 1012),\n",
       " ((\"d'accès\", ',', 'de', 'rectification', 'et'), 991),\n",
       " ((',', 'de', 'rectification', 'et', \"d'opposition\"), 991),\n",
       " (('de', 'rectification', 'et', \"d'opposition\", 'aux'), 991),\n",
       " (('rectification', 'et', \"d'opposition\", 'aux', 'données'), 991),\n",
       " (('et', \"d'opposition\", 'aux', 'données', 'vous'), 991),\n",
       " ((\"d'opposition\", 'aux', 'données', 'vous', 'concernant'), 991),\n",
       " (('aux', 'données', 'vous', 'concernant', 'en'), 991),\n",
       " (('.', 'Aujourd', '’', 'hui', ','), 967),\n",
       " (('les', 'newsletters', 'qui', 'vous', 'intéressent'), 950),\n",
       " (('newsletters', 'qui', 'vous', 'intéressent', '.'), 950),\n",
       " (('n', '’', 'y', 'a', 'pas'), 932),\n",
       " (('Votre', 'adresse', 'email', 'nous', 'sert'), 886),\n",
       " (('adresse', 'email', 'nous', 'sert', 'à'), 886),\n",
       " (('email', 'nous', 'sert', 'à', 'vous'), 886),\n",
       " (('nous', 'sert', 'à', 'vous', 'adresser'), 886),\n",
       " (('sert', 'à', 'vous', 'adresser', 'les'), 886),\n",
       " (('à', 'vous', 'adresser', 'les', 'newsletters'), 886),\n",
       " (('vous', 'adresser', 'les', 'newsletters', 'qui'), 886),\n",
       " (('adresser', 'les', 'newsletters', 'qui', 'vous'), 886),\n",
       " (('qui', 'vous', 'intéressent', '.', 'Vous'), 886),\n",
       " (('vous', 'intéressent', '.', 'Vous', 'disposez'), 886),\n",
       " (('intéressent', '.', 'Vous', 'disposez', \"d'un\"), 886),\n",
       " (('.', 'Vous', 'disposez', \"d'un\", 'droit'), 886),\n",
       " (('Vous', 'disposez', \"d'un\", 'droit', \"d'accès\"), 886),\n",
       " (('données', 'vous', 'concernant', 'en', 'vous'), 886),\n",
       " (('vous', 'concernant', 'en', 'vous', 'connectant'), 886),\n",
       " (('concernant', 'en', 'vous', 'connectant', 'à'), 886),\n",
       " (('en', 'vous', 'connectant', 'à', 'votre'), 886)]\n",
       " ** REMOVED REMAINING LINES FOR READABILITY ON GITHUB ** "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(fivegrams, key=lambda x: x[1], reverse=True)[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chains\n",
    "\n",
    "I will pass my corpus through [`markovify`](https://github.com/jsvine/markovify).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = markovify.Text(raw_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instaurez une atmosphère d'incompréhension, de défiance, j'avoue ne plus attribuer d’épreuves à la loi Informatique et Libertés du 6 janvier 1978 relative à l'informatique, aux fichiers et aux voies légales pour ramener le jeune Laurent, à seulement 38 ans, passent la plus corrompue de Poutine avec admiration.\n",
      "\n",
      "On a vu des personnes hébergées à l’hôtel, raconte Mamadou Ba.\n",
      "\n",
      "Il indique l'acidité par une zone marécageuse.\n",
      "\n",
      "De même, alors que toute nouvelle version du plaignant principal est-elle crédible?\n",
      "\n",
      "Sinon, je vais le garder en tête que les faucons sont la Chine et le Jiangsu.\n",
      "\n",
      "« Amorcée le 15 février 2018Dossier Faire parler de son père: décidemment la fréquentation des maîtres les plus consultés actuellement ? Le goulet de quelque façon que la tête du classement des pays ACP constitue un avantage considérable à nos ados, exhibant des corps était paralysé.\n",
      "\n",
      "» Si elle resurgit pendant les fêtes de cour, le personnel d'un édile lui est faite de la forme ultime du monde économique.\n",
      "\n",
      "Ils professaient un fort investissement des familles qui joue.\n",
      "\n",
      "Les Etats-Unis ont même été constatés par les premiers inventeurs des outils.\n",
      "\n",
      "Vous, vous n'avez participé que par une salve de missiles air-air russes frappent l’avion coréen.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(text_model.make_sentence(tries=100))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocates\n",
    "\n",
    "Finding differences in collocates in four different words that mean 'exit' in French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_enum = list(enumerate(nltk_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26445037"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All the verb forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortir_forms = set(('sortir sors sort sortons sortez sortent sorti '\n",
    "                    'sortis sortais sortait sortiez sortiez sortaient '\n",
    "                    'sortis sortit sortîtes sortîmes sortirent sortirai '\n",
    "                    'sortiras sortira sortirons sortirez sortiront '\n",
    "                    'sortirais sortirait sortirions sortiriez sortiraient '\n",
    "                    'sorte sortes sorte sortions sortiez sortent sortisse sortisses '\n",
    "                    'sortît sortissions sortissiez sortissent sortant').split())\n",
    "\n",
    "partir_forms = set(('partir pars part partons partez partent parti partis partais '\n",
    "                    'partait partions partiez partaient partîmes partirent '\n",
    "                    'partirai partiras partira partirons partirez partiront '\n",
    "                    'partirais partirait partirions partiriez partiraient '\n",
    "                    'parte partes parte partions partiez partisse partisses '\n",
    "                    'partît partissions partissiez partissent partant').split())\n",
    "\n",
    "quitter_forms = set(('quitter quitte quittes quitte quittons quittez quittent quittais '\n",
    "                     'quittais quittait quittai quittas quitta quittâmes quittâmes '\n",
    "                     'quittâtes quittèrent quitté quitterai quitteras quittera '\n",
    "                     'quitterons quitterez quitteron quitterais quitterais '\n",
    "                     'quitterait quitterions quitteriez quitteraient quitte '\n",
    "                     'quittes quittions quittiez quittent quittant quittasse '\n",
    "                     'quittasses quittât quittassions quittassiez quittassent').split())\n",
    "\n",
    "laisser_forms = set(('laisser laisse lasses laisse laissons laissez laissent laissais '\n",
    "                     'laissé laissait laissions laissiez laissaient laissai laissas '\n",
    "                     'laissa laissâmes laissâtes laissèresnt laisserai laisseras laissera '\n",
    "                     'laisserons laisserez laisseront laisserais laisserais laisserait '\n",
    "                     'laisserions laisseriez laisseraient laisse laisses laisse laissions '\n",
    "                     'laissiez laissasse laissesses laissât laissassions laissassiez '\n",
    "                     'laissassent laissant').split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finder(nltk_tokens, verb_forms, n_words):\n",
    "    \"\"\" pass in all the verb forms and return the context surrounding each verb\"\"\"\n",
    "    indexes = [i for i, word in enumerate(nltk_tokens) if word in verb_forms]\n",
    "    results = []\n",
    "    for i in indexes:\n",
    "        context = (nltk_tokens[i - n_words:i] +         # words to the left\n",
    "                  # ['__' + nltk_tokens[i] + '__'] +       # search word\n",
    "                  [nltk_tokens[i]] +                    # search word\n",
    "                  nltk_tokens[i + 1:i + n_words + 1])   # words to the right\n",
    "        results.append(context)\n",
    "    print('len of results', len(results))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find and store the four verbs and their contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of results 10484\n"
     ]
    }
   ],
   "source": [
    "sortir_concordance = finder(nltk_tokens, sortir_forms, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of results 22075\n"
     ]
    }
   ],
   "source": [
    "partir_concordance = finder(nltk_tokens, partir_forms, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of results 3886\n"
     ]
    }
   ],
   "source": [
    "quitter_concordance = finder(nltk_tokens, quitter_forms, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of results 8290\n"
     ]
    }
   ],
   "source": [
    "laisser_concordance = finder(nltk_tokens, laisser_forms, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_concordances = [sortir_concordance,\n",
    "                    partir_concordance,\n",
    "                    quitter_concordance,\n",
    "                    laisser_concordance]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the concordance results\n",
    "\n",
    "Modifying `spaCy` to tag for part of speech, but disabling `parser` and `ner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('fr', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10484"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sortir_concordance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNDESIRED = ['PUNCT', 'NUM'] # could add determiners 'DET' here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def framer(concordance_list):\n",
    "    \"\"\" takes the concordance list and \n",
    "        returns a list of relevant pairs\"\"\"\n",
    "    i = 0\n",
    "    results = []\n",
    "    for phrase in concordance_list:\n",
    "        spacyd = nlp(' '.join(phrase))\n",
    "\n",
    "        psd = [(token.text, token.pos_) for token in spacyd if \\\n",
    "               token.pos_ not in UNDESIRED]\n",
    "        results.append(psd)\n",
    "\n",
    "    i += 1\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with  0\n",
      "done with  1\n",
      "done with  2\n",
      "done with  3\n"
     ]
    }
   ],
   "source": [
    "tagged_concs = []\n",
    "for i, c in enumerate(all_concordances):\n",
    "    tagged_concs.append(framer(c))\n",
    "    print('done with ', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle the processed results\n",
    "\n",
    "This notebook is getting really big, and slow to run. Pickling allows me to close kernal, and reload the variables that I want at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tagged_verb_concs.pkl', 'wb') as f:\n",
    "    pickle.dump(tagged_concs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open the processed results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle # reopen after closing the kernal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tagged_verb_concs.pkl', 'rb') as f:\n",
    "    tagged_concs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Align, create, and display `FreqDists` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the format of `tagged_concs`:\n",
    "\n",
    "0. sortir_concordance\n",
    "    - phrase\n",
    "        - `(word, POS)`\n",
    "        - `(word, POS)`\n",
    "        - `(word, POS)`\n",
    "    - phrase\n",
    "        - `(word, POS)`\n",
    "        - `(word, POS)`\n",
    "        - `(word, POS)`\n",
    "1. partir_concordance\n",
    "    - phrase (etc.)\n",
    "2. quitter_concordance\n",
    "3. laisser_concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortir = tagged_concs[0]\n",
    "partir = tagged_concs[1]\n",
    "quitter = tagged_concs[2]\n",
    "laisser = tagged_concs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aligner(tagged_conc, verb_forms, display='pos'):\n",
    "    \"\"\" Pass in `tagged_conc` and `verb_forms` \n",
    "        process them and print the results. \n",
    "        \n",
    "        `display` can be set to `pos` or `word`\n",
    "        default for `display` == 'pos'\n",
    "        \"\"\"\n",
    "    \n",
    "    aligned = []\n",
    "    # iterate over phrases\n",
    "    for phrase in tagged_conc:\n",
    "        # iterate over items in phrase\n",
    "        for i, (word, pos) in enumerate(phrase):\n",
    "            # align words around node verb form\n",
    "            if word in verb_forms and pos == 'VERB':\n",
    "                context = {}\n",
    "                # select everything to the left\n",
    "                context['left'] = phrase[:i]\n",
    "                # select the found form of the verb\n",
    "                context['node'] = phrase[i]\n",
    "                # select everything to the right\n",
    "                context['right'] = phrase[i + 1:]\n",
    "                aligned.append(context)\n",
    "                \n",
    "    \"\"\" Pass in aligned searches and display the context \"\"\"\n",
    "    \n",
    "    # ************************************************************\n",
    "    # Build the lists\n",
    "    # ************************************************************\n",
    "    \n",
    "    # (l|r)_dist is the left or right distribution of words\n",
    "    l_dist = {}\n",
    "    # verb forms\n",
    "    node = []\n",
    "    # r_dist\n",
    "    r_dist = {}\n",
    "\n",
    "    # ************************************************************\n",
    "    # Organize the results\n",
    "    # ************************************************************\n",
    "\n",
    "    # `display` from kwargs\n",
    "    if display == 'pos':\n",
    "        n = 1\n",
    "    elif display == 'word':\n",
    "        n = 0\n",
    "    \n",
    "    CARE_ABOUT = ['NOUN' 'ADJ', 'PROPN']\n",
    "    \n",
    "    for phrase in aligned:\n",
    "        # left context `reversed` to work from right to left\n",
    "        for i, (word, pos) in reversed(list(enumerate(phrase['left']))):\n",
    "            choose = (word, pos)\n",
    "            if pos in CARE_ABOUT:\n",
    "                try:\n",
    "                    l_dist[i] += [choose[n]]\n",
    "                except KeyError:\n",
    "                    l_dist[i] = []\n",
    "                    l_dist[i] += [choose[n]]\n",
    "        \n",
    "        # save node words\n",
    "        node += [(phrase['node'])]\n",
    "\n",
    "        # right context    \n",
    "        for i, (word, pos) in enumerate(phrase['right']):\n",
    "            choose = (word, pos)\n",
    "            if pos in CARE_ABOUT:\n",
    "                try:\n",
    "                    r_dist[i] += [choose[n]]\n",
    "                except KeyError:\n",
    "                    r_dist[i] = []\n",
    "                    r_dist[i] += [choose[n]]\n",
    "    \n",
    "    # ************************************************************\n",
    "    # `FreqDist` of the left and right-hand contexts\n",
    "    # ************************************************************\n",
    "\n",
    "    \n",
    "    dists = [l_dist, r_dist]\n",
    "\n",
    "    # iterate over each r and l distribution\n",
    "    which = ['l_dist', 'r_dist']\n",
    "    \n",
    "    # `ch` type(int) is which channel, in a given `dist` or distribution\n",
    "    for ch, dist in enumerate(dists):\n",
    "        # go over the distribution for each position in order\n",
    "        total_dist = {}\n",
    "        \n",
    "        reverse = True # control flow (big to small)\n",
    "        # if the channel selected is `'r_dist'` then go small to big\n",
    "        if which[ch] == 'r_dist':\n",
    "            reverse = False\n",
    "\n",
    "        # build `FreqDist` objects from the `value_lists`\n",
    "        for index, value_list in sorted(dist.items(), reverse=reverse):\n",
    "            # print(f'{which[ch]}-{key}')\n",
    "            v_list = []\n",
    "            # `dist[index]` gets the appropriate `value_list` \n",
    "            for k, v in FreqDist(dist[index]).items():\n",
    "                v_list.append((k, v))\n",
    "                total_dist[f'{which[ch]}-{index}'] = v_list\n",
    "\n",
    "    # ************************************************************\n",
    "    # Display the results\n",
    "    # ************************************************************\n",
    "\n",
    "            print(f'{which[ch]}-{index}', sorted(v_list,\n",
    "                                               reverse=True,\n",
    "                                               key=lambda x: x[1])[:20])\n",
    "        if reverse:\n",
    "            print()\n",
    "            print('NODE WORD:', sorted(FreqDist(node).items(),\n",
    "                                       reverse=True,\n",
    "                                       key=lambda x: x[1])[:10])\n",
    "            print()\n",
    "#     return total_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l_dist-8 [('États-Unis', 1)]\n",
      "l_dist-7 [('iPad', 1)]\n",
      "l_dist-5 [('Carax', 1), ('Makhachkala', 1), ('Congrès', 1), ('ENA', 1), ('Cantorbéry', 1), ('Europe', 1), ('Monde', 1), ('Australie', 1), ('OCDE', 1), ('Russie', 1), ('UMP', 1)]\n",
      "l_dist-4 [('Allemagne', 3), ('iPad', 2), ('UMP', 2), ('Mercy', 1), ('Richet', 1), ('Anzhi', 1), ('Johnson', 1), ('Etats-Unis', 1), ('Clichy', 1), ('Agricole', 1), ('Européens', 1), ('USA', 1), ('Oury', 1), ('Baer', 1), ('Biarritz', 1), ('Bouches-du-Rhône', 1), ('Glénat', 1), ('Monde', 1), ('Croisette', 1), ('Beart', 1)]\n",
      "l_dist-3 [('France', 10), ('Grèce', 4), ('UMP', 3), ('Madagascar', 3), ('Aubry', 2), ('Bauer', 2), ('Pers', 2), ('Europe', 2), ('Hollywood', 2), ('Russie', 2), ('Manon', 2), ('Demy', 2), ('Allemagne', 2), ('Rwanda', 2), ('Blum', 1), ('JACKSON', 1), ('DSK', 1), ('PC', 1), ('résistance', 1), ('Nosferatu', 1)]\n",
      "l_dist-2 [('France', 13), ('’', 4), ('Vaincre', 4), ('Europe', 4), ('Amérique', 3), ('Jacques', 3), ('Jean', 2), ('Google', 2), ('Domenech', 2), ('Stéphanie', 2), ('Siège', 2), ('Jussiaux', 2), ('Suède', 2), ('États-Unis', 2), ('Kate', 2), ('FN', 2), ('Benoît', 2), ('Tignes', 2), ('Léon', 1), ('Michäel', 1)]\n",
      "l_dist-1 [('Europe', 9), ('France', 5), ('Jacques', 3), ('Patrick', 3), ('’', 3), ('n', 3), ('Paris', 3), ('Car', 3), ('Michel', 2), ('Christian', 2), ('Grèce', 2), ('Nevermind', 2), ('Julien', 2), ('Saint', 2), ('Gérard', 2), ('Mitterrand', 2), ('Youngbloood', 1), ('Pikifou', 1), ('Hollande', 1), ('Personne', 1)]\n",
      "l_dist-0 [('France', 5), ('Paris', 3), ('Grèce', 2), ('Denis', 2), ('Gabriel', 2), ('Jean', 2), ('Finlande', 2), ('Afrique', 2), ('Isabelle', 2), ('Christ', 2), ('Gérard', 2), ('Découvrez', 2), ('Gaulle', 2), ('Daniel', 2), ('Sortez', 2), ('Delaigue', 2), ('Rajaonarimampianina', 2), ('Hermès', 1), ('Méroé', 1), ('Nanar', 1)]\n",
      "\n",
      "NODE WORD: [(('sortir', 'VERB'), 2444), (('sorti', 'VERB'), 463), (('sortent', 'VERB'), 408), (('sort', 'VERB'), 259), (('sortant', 'VERB'), 250), (('sortira', 'VERB'), 156), (('sortait', 'VERB'), 122), (('sorte', 'VERB'), 91), (('sortez', 'VERB'), 55), (('sortaient', 'VERB'), 53)]\n",
      "\n",
      "r_dist-0 [('Laurent', 8), ('Jean', 6), ('Manchester', 3), ('Faure', 2), ('au', 2), ('Ellul', 2), ('PCF', 1), ('Lagardère', 1), ('Man', 1), ('Renault', 1), ('Racontez', 1), ('Patrick', 1), ('Rokossovsky', 1), ('Pierre', 1), ('Lavezzi', 1), ('Jack', 1), ('Marseille', 1), ('Ferguson', 1), ('Car', 1), ('Boeing', 1)]\n",
      "r_dist-1 [('France', 13), ('Gbagbo', 7), ('Michel', 5), ('Schengen', 5), ('United', 3), ('Gnassingbé', 2), ('Paris', 2), ('tu', 2), ('Tang', 2), ('BD', 2), ('Moyen', 2), ('Vatican', 2), ('paris', 2), ('Grèce', 2), ('États-Unis', 2), ('Polytechnique', 2), ('Yann', 2), ('prépas', 2), ('Christian', 1), ('CHRU', 1)]\n",
      "r_dist-2 [('Union', 7), ('Car', 3), ('France', 3), ('Europe', 3), ('Ferrand', 3), ('Knäcke', 2), ('Supertramp', 2), ('UE', 2), ('Japon', 2), ('Cyr', 2), ('Israël', 2), ('Age', 2), ('Libye', 2), ('Essec', 2), ('Hexagone', 2), ('Michael', 2), ('Universal', 2), ('Monde', 2), ('Rob', 1), ('Gregory', 1)]\n",
      "r_dist-3 [('France', 7), ('Guerre', 6), ('Ronald', 2), ('Égypte', 1), ('Ghislain', 1), ('re', 1), ('Toulon', 1), ('Domenech', 1), ('Tallandier', 1), ('ES', 1), ('William', 1), ('Armand', 1), ('Manuel', 1), ('Chanteur', 1), ('Harvard', 1), ('Jean', 1), ('Coeur', 1), ('Blonde', 1), ('Mondenard', 1), ('Bretagne', 1)]\n",
      "r_dist-4 [('France', 3), ('train-train', 3), ('Paris', 2), ('Saint', 2), ('Inde', 2), ('Jessé', 2), ('Peugeot', 1), ('Julliard', 1), ('Gaby', 1), ('Faravohitra', 1), ('Ethiopienne', 1), ('Bruxelles', 1), ('HP', 1), ('Collard', 1), ('Colin', 1), ('Alexandrie', 1), ('Noriega', 1), ('Europe', 1), ('Bourgois', 1), ('Minusca', 1)]\n",
      "r_dist-5 [('Union', 3), ('Euro', 1), ('Valentin', 1), ('Hôpital', 1), ('Seine-et-Marne', 1), ('Christophe', 1), ('Bouët', 1), ('SUICIDE', 1), ('Fernando', 1), ('Pauline', 1), ('Kuan', 1), ('Gallimard', 1), ('Nancy', 1), ('Antisocial', 1), ('Amiens', 1), ('Elysée', 1), ('Frédéric', 1), ('Austria', 1), ('Gagnaire', 1)]\n",
      "r_dist-6 [('Rufin', 1)]\n",
      "r_dist-7 [('France', 1)]\n"
     ]
    }
   ],
   "source": [
    "aligner(sortir, sortir_forms, display='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l_dist-7 [('Gaulle', 1)]\n",
      "l_dist-6 [('Dijoux', 1), ('Majesté', 1), ('Massy', 1), ('au', 1), ('Laissez', 1)]\n",
      "l_dist-5 [('Europe', 3), ('Seine', 2), ('Laure', 1), ('Charles', 1), ('France', 1), ('Loomis', 1), ('Evry', 1), ('Arnaud', 1), ('Cousteau', 1), ('Tirez', 1), ('Sénart', 1), ('Angkor', 1), ('Sarkozy', 1), ('Borloo', 1), ('PCF', 1)]\n",
      "l_dist-4 [('Hollandais', 2), ('Louis', 2), ('Kim', 1), ('Anne', 1), ('Occident', 1), ('Midi-Pyrénées', 1), ('Sartre', 1), ('Bazar', 1), ('Paris', 1), ('Mendès', 1), ('Ohio', 1), ('Edith', 1), ('Iran', 1), ('Road', 1), ('Rochette', 1), ('Point', 1), ('Fangio', 1), ('Sarkozy', 1), ('UE', 1), ('Lyon', 1)]\n",
      "l_dist-3 [('France', 18), ('Beltrame', 8), ('Paris', 8), ('Europe', 5), ('Chine', 4), ('Allemagne', 3), ('Laissez', 3), ('Kong', 2), ('Magnan', 2), ('Maurice', 2), ('FRÉQUENTES', 2), ('USA', 2), ('Kyoto', 2), ('Lyon', 2), ('Aborigènes', 2), ('Pompidou', 2), ('Quesada', 2), ('AQMI', 2), ('Gaulle', 2), ('Madagascar', 2)]\n",
      "l_dist-2 [('France', 15), ('Paris', 13), ('Arnaud', 8), ('Jean', 5), ('François', 5), ('Hugo', 4), ('Lyon', 3), ('Pierre', 3), ('Delhi', 3), ('Rungis', 3), ('Hong', 2), ('Gaulle', 2), ('Europe', 2), ('’', 2), ('Duchesne', 2), ('Assad', 2), ('Afrique', 2), ('Montauban', 2), ('Marciac', 2), ('Terre', 2)]\n",
      "l_dist-1 [('France', 13), ('Paris', 10), ('Philippe', 7), ('Billets', 7), ('Jean', 5), ('Allemagne', 4), ('Europe', 4), ('Versailles', 4), ('Kabila', 3), ('Michel', 3), ('Foka', 3), ('Ligue', 2), ('Porto', 2), ('Wilson', 2), ('Gbagbo', 2), ('Berlin', 2), ('Compas', 2), ('Sénégal', 2), ('Pierre', 2), ('Roubaix', 2)]\n",
      "l_dist-0 [('France', 10), ('Paris', 7), ('Europe', 3), ('Union', 3), ('Côte', 3), ('Jazz', 3), ('Alain', 3), ('Car', 2), ('au', 2), ('Laurent', 2), ('AFP', 2), ('Philippe', 2), ('Plat', 2), ('OGM', 2), ('Jean', 2), ('Joseph', 2), ('Péguy', 2), ('Frédéric', 2), ('Laissez', 2), ('Jeanson', 1)]\n",
      "\n",
      "NODE WORD: [(('partir', 'VERB'), 6247), (('parti', 'VERB'), 1199), (('partent', 'VERB'), 421), (('partant', 'VERB'), 284), (('partait', 'VERB'), 106), (('partez', 'VERB'), 66), (('partis', 'VERB'), 65), (('pars', 'VERB'), 57), (('partira', 'VERB'), 55), (('partaient', 'VERB'), 52)]\n",
      "\n",
      "r_dist-0 [('au', 9), ('Majesté', 2), ('Lui', 2), ('Sawaba', 2), ('Laissez', 2), ('Chevènement', 1), ('Maxime', 1), ('Ecologistes', 1), ('Barils', 1), ('Younes', 1), ('Kagame', 1), ('Catherine', 1), ('Baas', 1), ('Aujourd', 1), ('Procurez', 1), ('Varsovie', 1), ('Aulas', 1), ('Seriati', 1), ('Ménagez', 1), ('Michel', 1)]\n",
      "r_dist-1 [('France', 15), ('Etats-Unis', 8), ('Paris', 8), ('Europe', 8), ('Afrique', 6), ('Auschwitz', 5), ('Allemagne', 4), ('Drancy', 4), ('Angleterre', 3), ('Google', 3), ('Italie', 3), ('Belgique', 3), ('Marseille', 3), ('au', 3), ('Afghanistan', 3), ('Chine', 2), ('Caire', 2), ('Kinshasa', 2), ('Sénégal', 2), ('Amérique', 2)]\n",
      "r_dist-2 [('France', 10), ('Afrique', 7), ('Etats-Unis', 5), ('Canada', 5), ('Asie', 5), ('Allemagne', 5), ('Partager', 4), ('York', 4), ('Renaissance', 4), ('Bravo', 3), ('Jean', 3), ('Inde', 3), ('Aujourd', 3), ('Europe', 3), ('Paris', 3), ('Monde', 2), ('Georges', 2), ('Yiwu', 2), ('Undergraduate', 2), ('Brésil', 2)]\n",
      "r_dist-3 [('Panthéon', 8), ('Car', 4), ('France', 4), ('Paris', 4), ('Allemagne', 3), ('Jean', 3), ('Oui', 3), ('Chine', 3), ('Sud', 3), ('Protégée', 3), ('Mothron', 2), ('Claire', 2), ('Asie', 2), ('Y', 2), ('Hammarskjöld', 2), ('Figaro', 2), ('Russie', 2), ('Etats-Unis', 2), ('Afternoon', 2), ('Dakar', 2)]\n",
      "r_dist-4 [('France', 5), ('Amérique', 4), ('Irak', 3), ('Angleterre', 3), ('Lyon', 3), ('Peltier', 2), ('ADN', 2), ('Daniel', 2), ('Pierre', 2), ('Christ', 2), ('Etats-Unis', 2), ('Monde', 2), ('Allemagne', 2), ('Claude', 2), ('Mixel', 2), ('Tea', 2), ('Sicile', 2), ('Foucault', 2), ('Afrique', 2), ('Gambie', 1)]\n",
      "r_dist-5 [('Irak', 2), ('Chine', 2), ('Europe', 2), ('Jasmine', 1), ('Internet', 1), ('Mauguio', 1), ('Orsay', 1), ('Aviv', 1), ('Tel', 1), ('Square', 1), ('Woolf', 1), ('Age', 1), ('Idaho', 1), ('États-Unis', 1), ('Gaule', 1), ('Union', 1), ('Afrique', 1), ('Bouygues', 1), ('Agnès', 1), ('Forrest', 1)]\n",
      "r_dist-6 [('Oskar', 1), ('Aménophis', 1), ('Europe', 1), ('Evangile', 1), ('CDU', 1), ('Maqalé', 1)]\n",
      "r_dist-8 [('Kabila', 1)]\n"
     ]
    }
   ],
   "source": [
    "aligner(partir, partir_forms, display='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l_dist-6 [('Hahaha', 1), ('Océan', 1)]\n",
      "l_dist-5 [('Avastin', 1), ('Etats-Unis', 1), ('Loire', 1), ('UPF', 1), ('Amri', 1), ('Zita', 1), ('Poveda', 1), ('Jourdain', 1), ('Afghanistan', 1), ('Evra', 1), ('Macron', 1), ('Villepin', 1)]\n",
      "l_dist-4 [('Figaro', 1), ('Buffon', 1), ('Guibert', 1), ('Esther', 1), ('Fig', 1), ('Israel', 1), ('Express', 1), ('Haute', 1), ('ONU', 1), ('ÉGALITÉ', 1), ('Picamoles', 1), ('United', 1), ('Vinci', 1), ('Europe', 1), ('PC', 1), ('Unis', 1), ('Orient', 1), ('Durand', 1), ('Bourguiba', 1), ('Jos', 1)]\n",
      "l_dist-3 [('France', 8), ('Sarkozy', 4), ('Paris', 4), ('Brésil', 3), ('Chalosse', 2), ('Gage', 2), ('Mediaset', 2), ('Morinière', 2), ('Berlusconi', 2), ('Ligonnès', 2), ('Russie', 2), ('Gaulle', 2), ('Mexicaine', 2), ('Stone', 2), ('Rien', 2), ('Cambus', 2), ('Chirac', 2), ('Jean', 2), ('Europe', 2), ('Eric', 2)]\n",
      "l_dist-2 [('France', 4), ('’', 4), ('Bruxelles', 4), ('Chine', 3), ('Paris', 3), ('Paul', 3), ('Hitler', 3), ('Sarkozy', 3), ('Clermont-Ferrand', 2), ('Nicolas', 2), ('Brest', 2), ('Internet', 2), ('Louis', 2), ('François', 2), ('Inter', 2), ('Silvio', 2), ('Garonne', 2), ('Facebook', 2), ('Wagner', 2), ('Guzman', 2)]\n",
      "l_dist-1 [('France', 7), ('Paul', 5), ('Jean', 5), ('Dieu', 4), ('Internet', 3), ('Hollande', 3), ('François', 3), ('PS', 3), ('Europe', 3), ('Cayenne', 2), ('Bayern', 2), ('Nigeria', 2), ('Conseil', 2), ('Jacques', 2), ('Sochaux', 2), ('Terre', 2), ('Val-de-Marne', 2), ('Figaro', 2), ('Claude', 2), ('Michel', 2)]\n",
      "l_dist-0 [('Xavier', 5), ('France', 5), ('Jean', 4), ('Patrick', 4), ('François', 3), ('Europe', 2), ('Porsche', 2), ('Pierre', 2), ('Paris', 2), ('Fulbert', 2), ('Créteil', 2), ('Dieu', 2), ('Katumbi', 2), ('Hollande', 2), ('Shennawy', 2), ('Anglais', 2), ('Hongkongais', 2), ('Car', 2), ('Véxé', 2), ('Poutine', 2)]\n",
      "\n",
      "NODE WORD: [(('laisser', 'VERB'), 2480), (('laisse', 'VERB'), 1982), (('laissé', 'VERB'), 1368), (('laissant', 'VERB'), 634), (('laissent', 'VERB'), 508), (('laissait', 'VERB'), 163), (('laissera', 'VERB'), 120), (('laisserait', 'VERB'), 55), (('laissaient', 'VERB'), 46), (('laissez', 'VERB'), 42)]\n",
      "\n",
      "r_dist-0 [('au', 2), ('Stefan', 2), ('Liliane', 2), ('Arbeloa', 1), ('Mamadou', 1), ('Royal', 1), ('Bokassa', 1), ('Carax', 1), ('Françoise', 1), ('Fabien', 1), ('Jonquet', 1), ('Charlotte', 1), ('Grace', 1), ('Djokhar', 1), ('t', 1), ('Rogov', 1), ('Guy', 1), ('Abbi', 1), ('Danielle', 1), ('Hamid', 1)]\n",
      "r_dist-1 [('France', 9), ('Italie', 4), ('Dieu', 3), ('Paul', 3), ('Marcel', 2), ('au', 2), ('Sénégal', 2), ('Berlusconi', 2), ('François', 2), ('Paris', 2), ('Jackson', 2), ('Russie', 2), ('Lui', 2), ('Michel', 2), ('Matmut', 2), ('Albert', 2), ('Champagne', 2), ('FN', 2), ('Bettencourt', 2), ('Re', 1)]\n",
      "r_dist-2 [('France', 5), ('’', 4), ('Dieu', 4), ('Nicolas', 3), ('au', 2), ('Finette', 2), ('Christ', 2), ('Etats-Unis', 2), ('Michel', 2), ('Patrick', 2), ('Mail', 2), ('oeuvre', 2), ('Jean', 2), ('Charlotte', 2), ('Madagascar', 2), ('Stéphane', 1), ('Brassens', 1), ('Poulain', 1), ('Monseigneurestlevotre', 1), ('Barzagli', 1)]\n",
      "r_dist-3 [('France', 5), ('Chine', 3), ('Afrique', 3), ('Sarkozy', 3), ('François', 2), ('Jean', 2), ('Marcel', 2), ('Sud', 2), ('Paris', 2), ('Lorsqu', 2), ('Londres', 2), ('Russie', 2), ('Henry', 2), ('Malgaches', 2), ('Fouks', 1), ('Real', 1), ('Remi', 1), ('FN', 1), ('Marseille', 1), ('Cimade', 1)]\n",
      "r_dist-4 [('Dieu', 4), ('Troie', 2), ('Etats-Unis', 2), ('Bettencourt', 1), ('Compiègne', 1), ('Tournesol', 1), ('McCaslin', 1), ('Jacques', 1), ('Internet', 1), ('Martinot', 1), ('LOUIS', 1), ('Elysée', 1), ('Sarkozy', 1), ('Agricole', 1), ('Monde', 1), ('Ferguson', 1), ('Arte', 1), ('Gironde', 1), ('Ahmed', 1), ('FN', 1)]\n",
      "r_dist-5 [('Aillagon', 1), ('Afrique', 1), ('Liliane', 1), ('Oran', 1), ('France', 1), ('Majorque', 1), ('Union', 1), ('Medica', 1), ('Sarkozy', 1), ('Jing', 1), ('Mondial', 1), ('Voyage', 1), ('Jean', 1), ('Neandertal', 1), ('Nétanyahou', 1)]\n",
      "r_dist-6 [('Idriss', 1), ('Iliad', 1), ('Auxerre', 1), ('Qaida', 1), ('Louis', 1)]\n"
     ]
    }
   ],
   "source": [
    "aligner(laisser, laisser_forms, display='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l_dist-5 [('Oui', 1), ('Laâge', 1), ('Blake', 1), ('Gomez', 1)]\n",
      "l_dist-4 [('Allemagne', 2), ('Turquie', 1), ('PSA', 1), ('Nagui', 1), ('France', 1), ('Iran', 1), ('Chirac', 1), ('B.', 1), ('Kadhafi', 1), ('Jospin', 1), ('Aquitaine', 1), ('Kenpo', 1), ('Ivoire', 1), ('Octavio', 1), ('Selena', 1), ('Samuel', 1), ('CIA', 1), ('Elysée', 1), ('Wehrmacht', 1)]\n",
      "l_dist-3 [('Allemagne', 4), ('Dempsey', 3), ('Preynat', 2), ('France', 2), ('Paris', 2), ('Kadhafi', 2), ('Lionel', 2), ('Gear', 2), ('Blanches', 2), ('Walker', 2), ('Marci', 1), ('Espanyol', 1), ('Staël', 1), ('Carvalho', 1), ('Sanogo', 1), ('Daw', 1), ('Thierry', 1), ('Bayern', 1), ('Raynaud', 1), ('Laurioux', 1)]\n",
      "l_dist-2 [('France', 7), ('François', 5), ('Patrick', 3), ('’', 3), ('Facebook', 3), ('Bernard', 2), ('ONU', 2), ('Nicolas', 2), ('Paul', 2), ('Jamais', 2), ('Trévedy', 2), ('Chine', 2), ('Beaudoin', 2), ('Sétois', 1), ('Scampia', 1), ('Ottomans', 1), ('Lolo', 1), ('Biya', 1), ('Love', 1), ('Poursin', 1)]\n",
      "l_dist-1 [('Paris', 4), ('Jean', 4), ('Ali', 3), ('Paul', 3), ('Anne', 2), ('Lorsqu', 2), ('Côte', 2), ('Villepin', 2), ('Irak', 2), ('Claude', 2), ('City', 2), ('Henok', 2), ('Serge', 2), ('Fée', 2), ('Cassingena', 2), ('Villars', 1), ('Costner', 1), ('REUTERS', 1), ('Idriss', 1), ('Kevin', 1)]\n",
      "l_dist-0 [('France', 8), ('François', 5), ('Paris', 5), ('Nicolas', 3), ('Bernard', 3), ('Kevin', 2), ('Louis', 2), ('Jacques', 2), ('Gérard', 2), ('Allemagne', 2), ('Laurent', 2), ('Billy', 2), ('Santiago', 2), ('Kadhafi', 2), ('Claude', 2), ('Besson', 1), ('Barcelone', 1), ('Ligue', 1), ('Ercole', 1), ('Darfour', 1)]\n",
      "\n",
      "NODE WORD: [(('quitter', 'VERB'), 1344), (('quitté', 'VERB'), 1002), (('quitte', 'VERB'), 213), (('quittant', 'VERB'), 109), (('quittera', 'VERB'), 80), (('quittent', 'VERB'), 77), (('quittait', 'VERB'), 53), (('quitterait', 'VERB'), 14), (('quittez', 'VERB'), 13), (('quitta', 'VERB'), 9)]\n",
      "\n",
      "r_dist-0 [('Paris', 20), ('Rome', 7), ('Londres', 4), ('Prague', 4), ('Bordeaux', 4), ('Microsoft', 4), ('Lorient', 3), ('Kinshasa', 3), ('Mossoul', 3), ('Oran', 2), ('Cologne', 2), ('Hollywood', 2), ('Varanasi', 2), ('Charlie', 2), ('Washington', 2), ('Kaboul', 2), ('Lyon', 2), ('Dakar', 2), ('Goa', 2), ('Universal', 2)]\n",
      "r_dist-1 [('France', 57), ('Afrique', 8), ('UE', 7), ('Élysée', 7), ('Allemagne', 6), ('Union', 5), ('Côte', 5), ('Europe', 5), ('Irak', 5), ('Elysée', 5), ('Angleterre', 5), ('Libye', 4), ('York', 4), ('Burkina', 4), ('Guinée', 3), ('Californie', 3), ('Russie', 3), ('Argentine', 3), ('Somalie', 2), ('Saint', 2)]\n",
      "r_dist-2 [('Madrid', 2), ('Car', 2), ('Milan', 2), ('Strasbourg', 2), ('Faso', 2), ('Salaam', 2), ('Jean', 2), ('Beaudoin', 2), ('Élysée', 1), ('ZUP', 1), ('Latino', 1), ('Paris', 1), ('Pompeo', 1), ('Joseph', 1), ('Rires', 1), ('Cœur', 1), ('Atacama', 1), ('Freiburg', 1), ('Etienne', 1), ('Samba', 1)]\n",
      "r_dist-3 [('Grenoble', 5), ('ZEP', 4), ('Sud', 3), ('Ivoire', 3), ('OQTF', 3), ('Londres', 3), ('n', 2), ('Conseil', 2), ('Séminaire', 2), ('Azur', 2), ('Padoue', 2), ('François', 2), ('Verts', 1), ('Vegas', 1), ('Libreville', 1), ('Kabila', 1), ('Rémy', 1), ('Zuckerberg', 1), ('Pantin', 1), ('Malaisie', 1)]\n",
      "r_dist-4 [('Canada', 3), ('Office', 2), ('France', 2), ('Washington', 2), ('Angleterre', 2), ('Allemagne', 2), ('Island', 2), ('Hollande', 2), ('Kenya', 1), ('Suède', 1), ('Argentine', 1), ('Comminges', 1), ('Jiangxi', 1), ('Grüsch', 1), ('Kivu', 1), ('Tahsin', 1), ('Aston', 1), ('Briand', 1), ('Aude', 1), ('Elysée', 1)]\n",
      "r_dist-5 [('UE', 3), ('Barça', 1), ('Gradignan', 1), ('Europe', 1), ('Hebergé', 1), ('Steklov', 1), ('Aisne', 1), ('Ouest', 1), ('Hillary', 1), ('Italie', 1), ('Gauguin', 1)]\n",
      "r_dist-6 [('Toulon', 1), ('Paris', 1)]\n"
     ]
    }
   ],
   "source": [
    "aligner(quitter, quitter_forms, display='word')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER Analysis\n",
    "\n",
    "Frequency distribution of named entities for each domain in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import justext \n",
    "import spacy\n",
    "import re\n",
    "nlp = spacy.load('fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAGE_GLOB = glob('/Users/Kyle/Documents/archives/Winter 2018/'\n",
    "                 'corpus-linguistics/french-news-html/'\n",
    "                 'goscraper/20mar-links/*.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(page):\n",
    "    \"\"\"This function takes an html page from a glob, for exmaple,\n",
    "       and reads it and uses the justext module remove all boilerplate\"\"\"\n",
    "\n",
    "    # reads the file\n",
    "    page_string = open(page, 'rb').read()\n",
    "    # creates a justext object\n",
    "    paragraphs = justext.justext(page_string,\n",
    "                                 justext.get_stoplist(\"French\"))\n",
    "    pageText = ''\n",
    "    # if not boilerplate, adds to `pageText`\n",
    "    for p in paragraphs:\n",
    "        if not p.is_boilerplate:\n",
    "            pageText += p.text + ' '\n",
    "    return pageText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link Index\n",
    "\n",
    "Create an index link to scrape number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_file = open('golang-link-index.txt', 'r').readlines()\n",
    "\n",
    "url_index = []\n",
    "for line in index_file:\n",
    "    divided = line.split()\n",
    "    url = re.sub(',', '', divided[0])\n",
    "    index = re.findall(r'\\d{2}mar-links/(\\d+\\.html)', divided[1])[0]\n",
    "    url_index.append((url, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(page_number):\n",
    "    \"\"\" take the indexed filename and return the actual link \"\"\"\n",
    "    url = [url for url, index in url_index if index == page_number]\n",
    "    return url[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def domain_analyze(url):\n",
    "    \"\"\" determines what domain the url is from \"\"\"\n",
    "    \n",
    "    re_domain = r'https?://((?:www\\.)?.*?\\..*?)/'\n",
    "    domain = re.search(re_domain, url).group(1)\n",
    "\n",
    "    return domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticleData:\n",
    "    \"\"\" A place to store all of the things we find in each article \n",
    "    \n",
    "        `nes` is 'named enetities' \"\"\"\n",
    "    def __init__(self, article_text, url, domain, entity_labels, nes):\n",
    "        self.article_text = article_text\n",
    "        self.url = url\n",
    "        self.domain = domain\n",
    "        self.entity_labels = entity_labels\n",
    "        self.nes = nes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n"
     ]
    }
   ],
   "source": [
    "# `PAGE_GLOB` is a list of scraped pages `1.html`, `2.html`, etc.\n",
    "# `indexed_path_html` is a file path to the stored html on my computer\n",
    "result_list = []\n",
    "\n",
    "TOTAL = len(PAGE_GLOB)\n",
    "i = 0\n",
    "for indexed_path_html in PAGE_GLOB:\n",
    "    # get text\n",
    "    article_text = get_text(indexed_path_html)\n",
    "    # get the page number out of the path\n",
    "    page_number = re.findall(r'(\\d+\\.html)', indexed_path_html)[0]    \n",
    "    # get the url\n",
    "    url = get_url(page_number)\n",
    "    # get the domain\n",
    "    domain = domain_analyze(url)\n",
    "\n",
    "    # analyze the text\n",
    "    doc = nlp(article_text)\n",
    "    \n",
    "    # store the labels\n",
    "    labels = []\n",
    "    nes = []\n",
    "    \n",
    "    # get the named entities\n",
    "    for ent in doc.ents:\n",
    "        labels.append(ent.label_)\n",
    "        nes.append(ent.text)\n",
    "    \n",
    "    Results = ArticleData(article_text, url, domain, labels, nes)\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "        \n",
    "    i += 1\n",
    "    \n",
    "    result_list.append(Results)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23257"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('french_corpus_info.pkl', 'wb') as f:\n",
    "    pickle.dump(result_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('french_corpus_info.pkl', 'rb') as fa:\n",
    "    asdf = pickle.load(fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23257"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(asdf) # proves that the pickle worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article_text, url, domain, entity_labels, nes\n",
    "\n",
    "domain_distribution = {}\n",
    "\n",
    "for Res in result_list:\n",
    "    if Res.domain not in domain_distribution.keys():\n",
    "        domain_distribution[Res.domain] = 1    \n",
    "    else:\n",
    "        domain_distribution[Res.domain] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "561"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(domain_distribution.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most common named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7168 www.lemonde.fr [('\\n', 130153), ('Le Monde.fr', 43052), ('Découvrez', 21580), ('CGV', 21466), ('–', 16523), ('Monde\\n|', 14476), ('France', 11790), ('Etat', 10478), ('Paris', 8773), ('\\n|\\n', 7452), ('Etats-Unis', 6474)]\n",
      "\n",
      "2348 www.lefigaro.fr [('\\n', 40859), ('France', 8927), ('la France', 6543), ('Paris', 5029), ('Français', 4750), ('Jean', 2661), ('Europe', 2583), ('Etat', 2523), ('Russie', 2451), ('État', 2163), ('Allemagne', 2052)]\n",
      "\n",
      "740 www.liberation.fr [('C’', 2796), ('France', 1722), ('Paris', 1606), ('AFP', 1446), ('J’', 1443), ('m’', 1359), ('Etat', 1272), ('la France', 954), ('jusqu’', 846), ('Français', 837), ('–', 801)]\n",
      "\n",
      "727 www.futura-sciences.com [('Moteur de recherche', 1297), ('Futura Publié', 888), ('Futura', 864), ('la Terre', 729), ('Vous avez', 330), ('Terre', 330), ('France', 330), ('États-Unis', 318), ('Modifié', 306), ('Lune', 297), ('CO2', 294)]\n",
      "\n",
      "609 www.sudouest.fr [('C’', 1420), ('Bordeaux', 987), ('J’', 781), ('Jean', 774), ('jusqu’', 642), ('–', 539), ('France', 477), ('m’', 354), ('Paris', 349), ('Gironde', 342), ('de France', 276)]\n",
      "\n",
      "513 www.lexpress.fr [('\\n', 3930), ('France', 1326), ('Paris', 984), ('Etat', 751), ('Jean', 687), ('la France', 684), (\"L'Express\", 498), ('Français', 435), ('Etats-Unis', 359), ('Dieu', 255), ('de France', 246)]\n",
      "\n",
      "501 www.huffingtonpost.fr [('France', 399), ('Facebook', 390), ('HuffPost', 379), ('Paris', 294), ('–', 261), ('C’', 258), ('Etat', 234), ('Twitter', 204), ('m’', 195), ('la France', 186), ('Jean', 186)]\n",
      "\n",
      "498 www.lesechos.fr [('Liberté', 1486), ('loi Informatique', 1471), ('France', 1056), ('Paris', 768), ('Etats-Unis', 555), ('Etat', 555), ('Jean', 549), ('Europe', 435), ('la France', 399), ('Français', 363), ('Internet', 324)]\n",
      "\n",
      "463 www.parismatch.com [('C’', 1643), ('J’', 1290), ('m’', 1068), ('Paris', 1002), ('–', 845), ('\\n', 618), ('Jean', 477), ('jusqu’', 468), ('France', 450), ('Match|\\n', 408), ('Etat', 399)]\n",
      "\n",
      "416 www.la-croix.com [('–', 2884), ('C’', 2333), ('Église', 2249), ('Dieu', 2112), ('\\n', 1487), ('France', 1479), ('Jean', 1232), ('Paris', 1146), ('jusqu’', 1002), ('J’', 879), ('État', 834)]\n",
      "\n",
      "336 www.jeuneafrique.com [('–', 1429), ('État', 964), ('C’', 813), ('Afrique', 594), ('Côte d’Ivoire', 486), ('J’', 420), ('jusqu’', 414), ('Mali', 399), ('France', 375), ('Paris', 351), ('F CFA', 351)]\n",
      "\n",
      "315 www.ladepeche.fr [('\\n', 3510), ('Toulouse', 730), ('France', 435), ('Jean', 399), ('la France', 228), ('Paris', 225), ('de France', 198), ('Noël', 195), ('Napoléon', 159), ('Europe', 150), ('Garonne', 150)]\n",
      "\n",
      "312 www.lepoint.fr [('\\n', 1877), ('France', 330), ('Paris', 276), ('Apple', 237), ('Jean', 231), ('la France', 210), ('État', 210), ('Français', 141), ('États-Unis', 132), ('président de la République', 105), ('Europe', 102)]\n",
      "\n",
      "311 www.slate.fr [('\\n', 29724), ('C’', 730), ('France', 525), ('Gage', 414), ('Etats-Unis', 408), ('Facebook', 378), ('Etat', 360), ('la France', 321), ('Google', 288), ('–', 285), ('Paris', 276)]\n",
      "\n",
      "297 www.rfi.fr [('\\n', 6380), ('RFI', 2209), ('Bienvenue !', 1202), ('\\nRègles', 991), ('Le Partenaire', 991), ('Contenu', 991), ('Partenaire', 991), ('Republier', 883), ('Etat', 752), ('Consultez', 610), ('Votre', 604)]\n",
      "\n",
      "280 www.capital.fr [('France', 698), ('–', 651), ('C’', 587), ('Jean', 441), ('jusqu’', 390), ('Paris', 372), ('Faire', 324), ('Montez', 294), ('Changez de vie !', 291), ('Redonnez', 291), ('Management mensuel', 291)]\n",
      "\n",
      "256 www.20minutes.fr [('\\n', 315), ('C’', 291), ('France', 291), ('Paris', 288), ('Français', 171), ('Etats-Unis', 156), ('la France', 117), ('J’', 117), ('Minutes', 114), ('Jean', 104), ('Internet', 99)]\n",
      "\n",
      "240 www.contrepoints.org [('–', 1365), ('État', 1362), ('C’', 1005), ('France', 706), ('\\n', 534), ('jusqu’', 472), ('États-Unis', 444), ('la France', 375), ('Etat', 369), ('Partagez', 360), ('Hayek', 336)]\n",
      "\n",
      "210 www.europe1.fr [('France', 232), ('Français', 201), ('C’', 168), ('Europe', 168), ('Europe 1', 165), ('Etat', 144), ('la France', 138), ('PSG', 138), ('Paris', 132), ('de France', 129), ('Jean', 129)]\n",
      "\n",
      "202 sante.lefigaro.fr [('\\n', 1243), ('Société du Figaro', 604), ('France', 228), ('Le Figaro Santé', 219), ('C', 78), ('HTA', 69), ('IRM', 69), ('B', 60), ('Paris', 51), ('ASR', 42), ('A', 39)]\n",
      "\n",
      "194 www.francetvinfo.fr [('France', 378), ('AFP', 300), ('Paris', 234), ('Etats-Unis', 186), ('Francetv', 171), ('Etat', 168), ('Français', 156), ('|', 156), ('–', 141), ('Jean', 135), ('de France', 126)]\n",
      "\n",
      "164 www.telerama.fr [('\\n', 849), ('–', 714), ('Paris', 257), ('France', 200), ('C’', 174), ('Jean', 153), ('Etats-Unis', 135), ('Etat', 129), ('Foucault', 123), ('Truffaut', 111), ('Céline', 105)]\n",
      "\n",
      "144 www.letudiant.fr [('–', 401), ('Paris', 390), ('Gérer', 339), ('\\n', 293), ('France', 281), ('C’', 174), ('HEC', 150), ('Jean', 135), ('J’', 129), ('Répondre Signaler', 117), ('m’', 111)]\n",
      "\n",
      "130 madame.lefigaro.fr [('\\n', 876), ('C’', 290), ('J’', 280), ('–', 228), ('Société du Figaro', 168), ('\\ncommerciale', 168), ('Informatique', 168), ('m’', 168), ('Paris', 159), ('Newsletters', 135), ('jusqu’', 99)]\n",
      "\n",
      "127 www.ledauphine.com [('C’', 231), ('DL', 231), ('Partager', 154), ('J’', 150), ('jusqu’', 108), ('Photo', 105), ('Alpes', 96), ('Jean', 93), ('France', 88), ('Photo DL', 78), ('Photos DL', 72)]\n",
      "\n",
      "125 next.liberation.fr [('C’', 632), ('J’', 387), ('Paris', 357), ('m’', 276), ('France', 171), ('jusqu’', 135), ('Charlotte', 105), ('Jean', 99), ('Etats-Unis', 93), ('\\n', 84), ('Etat', 84)]\n",
      "\n",
      "117 www.lesinrocks.com [('C’', 441), ('–', 355), ('J’', 311), ('Paris', 310), ('France', 201), ('Johnny', 195), ('Facebook', 180), ('Jean', 162), ('Booba', 159), ('m’', 150), ('Etats-Unis', 126)]\n",
      "\n",
      "108 www.lejdd.fr [('\\n', 812), ('J’', 396), ('C’', 366), ('m’', 342), ('Partager', 292), ('Reuters', 282), ('Paris', 237), ('Jean', 234), ('France', 207), ('–', 168), ('Etat', 102)]\n",
      "\n",
      "101 www.gala.fr [('Gala', 243), ('C’', 222), ('J’', 168), ('–', 159), ('Paris', 144), ('m’', 108), ('Vidéo Biographie', 84), ('Jean', 66), ('France', 60), ('Bernard', 57), ('S. M.', 54)]\n",
      "\n",
      "100 www.marieclaire.fr [('C’', 267), ('J’', 201), ('m’', 165), ('Vos', 160), ('Adresse', 151), ('Vous De Adresse', 148), ('\\n', 143), ('Ça', 84), ('France', 72), ('–', 72), ('Rires', 57)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "most_common_nes = {}\n",
    "for domain in domain_distribution.keys():\n",
    "    for Res in result_list:\n",
    "        if Res.domain == domain:\n",
    "            if domain not in most_common_nes.keys():\n",
    "                most_common_nes[domain] = Res.nes\n",
    "            else:\n",
    "                most_common_nes[domain] += Res.nes\n",
    "\n",
    "# take the 30 most common domains\n",
    "most_common_domains = FreqDist(domain_distribution).most_common(30)    \n",
    "\n",
    "for domain, count in most_common_domains:\n",
    "    for domain2, nes in most_common_nes.items():\n",
    "        if domain == domain2:\n",
    "            print(count, domain, FreqDist(nes).most_common(11))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most common categories of NEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7168 www.lemonde.fr [('LOC', 164805), ('PER', 113350), ('ORG', 67072), ('MISC', 57506)]\n",
      "2348 www.lefigaro.fr [('LOC', 73784), ('PER', 53645), ('MISC', 30309), ('ORG', 29653)]\n",
      "740 www.liberation.fr [('LOC', 16917), ('PER', 12913), ('ORG', 7800), ('MISC', 7309)]\n",
      "727 www.futura-sciences.com [('LOC', 5383), ('PER', 4079), ('MISC', 3349), ('ORG', 2019)]\n",
      "609 www.sudouest.fr [('LOC', 9432), ('PER', 7176), ('MISC', 3041), ('ORG', 2184)]\n",
      "513 www.lexpress.fr [('PER', 13015), ('LOC', 12176), ('MISC', 5729), ('ORG', 4425)]\n",
      "501 www.huffingtonpost.fr [('PER', 5118), ('LOC', 4279), ('MISC', 3216), ('ORG', 2541)]\n",
      "498 www.lesechos.fr [('LOC', 8545), ('PER', 6344), ('ORG', 4228), ('MISC', 2854)]\n",
      "463 www.parismatch.com [('PER', 11843), ('LOC', 9500), ('MISC', 4522), ('ORG', 2613)]\n",
      "416 www.la-croix.com [('LOC', 20074), ('PER', 15622), ('MISC', 9077), ('ORG', 6129)]\n",
      "336 www.jeuneafrique.com [('LOC', 8961), ('PER', 6654), ('ORG', 2975), ('MISC', 2481)]\n",
      "315 www.ladepeche.fr [('LOC', 6269), ('PER', 4666), ('MISC', 2483), ('ORG', 1774)]\n",
      "312 www.lepoint.fr [('PER', 3943), ('LOC', 3787), ('MISC', 1747), ('ORG', 1427)]\n",
      "311 www.slate.fr [('LOC', 11631), ('MISC', 8039), ('PER', 6098), ('ORG', 2726)]\n",
      "297 www.rfi.fr [('LOC', 9205), ('PER', 5969), ('MISC', 4411), ('ORG', 3135)]\n",
      "280 www.capital.fr [('PER', 4694), ('LOC', 4331), ('MISC', 2332), ('ORG', 2235)]\n",
      "256 www.20minutes.fr [('LOC', 2749), ('PER', 2018), ('MISC', 1152), ('ORG', 1033)]\n",
      "240 www.contrepoints.org [('LOC', 5531), ('PER', 4338), ('MISC', 2392), ('ORG', 2058)]\n",
      "210 www.europe1.fr [('LOC', 2407), ('PER', 2127), ('MISC', 1111), ('ORG', 1097)]\n",
      "202 sante.lefigaro.fr [('LOC', 864), ('ORG', 757), ('MISC', 584), ('PER', 397)]\n",
      "194 www.francetvinfo.fr [('LOC', 2937), ('PER', 2043), ('ORG', 1335), ('MISC', 1294)]\n",
      "164 www.telerama.fr [('PER', 4333), ('LOC', 2542), ('MISC', 1940), ('ORG', 981)]\n",
      "144 www.letudiant.fr [('LOC', 2476), ('PER', 1973), ('ORG', 1611), ('MISC', 1193)]\n",
      "130 madame.lefigaro.fr [('PER', 2137), ('LOC', 1803), ('MISC', 1107), ('ORG', 587)]\n",
      "127 www.ledauphine.com [('LOC', 1824), ('PER', 1282), ('MISC', 589), ('ORG', 454)]\n",
      "125 next.liberation.fr [('PER', 3273), ('LOC', 2282), ('MISC', 1383), ('ORG', 655)]\n",
      "117 www.lesinrocks.com [('PER', 3095), ('LOC', 2019), ('MISC', 1575), ('ORG', 930)]\n",
      "108 www.lejdd.fr [('PER', 2743), ('LOC', 2338), ('MISC', 1046), ('ORG', 944)]\n",
      "101 www.gala.fr [('PER', 1725), ('LOC', 981), ('MISC', 698), ('ORG', 291)]\n",
      "100 www.marieclaire.fr [('PER', 987), ('MISC', 787), ('LOC', 630), ('ORG', 299)]\n"
     ]
    }
   ],
   "source": [
    "most_common_nes = {}\n",
    "for domain in domain_distribution.keys():\n",
    "    for Res in result_list:\n",
    "        if Res.domain == domain:\n",
    "            if domain not in most_common_nes.keys():\n",
    "                most_common_nes[domain] = Res.entity_labels\n",
    "            else:\n",
    "                most_common_nes[domain] += Res.entity_labels\n",
    "\n",
    "most_common_domains = FreqDist(domain_distribution).most_common(30)    \n",
    "\n",
    "for domain, count in most_common_domains:\n",
    "    for domain2, nes in most_common_nes.items():\n",
    "        if domain == domain2:\n",
    "            print(count, domain, FreqDist(nes).most_common(11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average article length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7168 www.lemonde.fr            5660.9189453125     \n",
      " 2348 www.lefigaro.fr           9214.779386712094   \n",
      "  740 www.liberation.fr         6408.097297297298   \n",
      "  727 www.futura-sciences.com   3276.977991746905   \n",
      "  609 www.sudouest.fr           3494.630541871921   \n",
      "  513 www.lexpress.fr           8097.479532163743   \n",
      "  501 www.huffingtonpost.fr     3886                \n",
      "  498 www.lesechos.fr           5606.31124497992    \n",
      "  463 www.parismatch.com        5486.088552915767   \n",
      "  416 www.la-croix.com          12341.701923076924  \n",
      "  336 www.jeuneafrique.com      5378.440476190476   \n",
      "  315 www.ladepeche.fr          5472.355555555556   \n",
      "  312 www.lepoint.fr            4192.75             \n",
      "  311 www.slate.fr              7220.5337620578775  \n",
      "  297 www.rfi.fr                5564.461279461279   \n",
      "  280 www.capital.fr            6249.425            \n",
      "  256 www.20minutes.fr          3051.62109375       \n",
      "  240 www.contrepoints.org      9598.045833333334   \n",
      "  210 www.europe1.fr            2955.095238095238   \n",
      "  202 sante.lefigaro.fr         2782.3564356435645  \n",
      "  194 www.francetvinfo.fr       3996.7319587628867  \n",
      "  164 www.telerama.fr           6118.1890243902435  \n",
      "  144 www.letudiant.fr          6080.291666666667   \n",
      "  130 madame.lefigaro.fr        4376.615384615385   \n",
      "  127 www.ledauphine.com        2926.1259842519685  \n",
      "  125 next.liberation.fr        6478.944            \n",
      "  117 www.lesinrocks.com        6733.717948717948   \n",
      "  108 www.lejdd.fr              5412.925925925926   \n",
      "  101 www.gala.fr               3049.3960396039606  \n",
      "  100 www.marieclaire.fr        4849.61             \n"
     ]
    }
   ],
   "source": [
    "most_common_nes = {}\n",
    "for domain in domain_distribution.keys():\n",
    "    for Res in result_list:\n",
    "        if Res.domain == domain:\n",
    "            if domain not in most_common_nes.keys():\n",
    "                most_common_nes[domain] = [len(Res.article_text)]\n",
    "            else:\n",
    "                most_common_nes[domain] += [len(Res.article_text)]\n",
    "\n",
    "most_common_domains = FreqDist(domain_distribution).most_common(30)    \n",
    "\n",
    "for domain, count in most_common_domains:\n",
    "    for domain2, lengths in most_common_nes.items():\n",
    "        if domain == domain2:\n",
    "            print('{:5} {:<25} {:<20}'.format(count, domain, mean(lengths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average TTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttr(article_text):\n",
    "    \"\"\" Returns the nltk-word_tokenized type to token ratio of the article \"\"\"\n",
    "    tokens = word_tokenize(article_text)\n",
    "    types = set(tokens)\n",
    "    \n",
    "    try:\n",
    "        ttr = len(types) / len(tokens)\n",
    "    except ZeroDivisionError:\n",
    "        ttr = 0\n",
    "\n",
    "    return ttr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7168 www.lemonde.fr            0.4661329469209219  \n",
      " 2348 www.lefigaro.fr           0.43289061416688746 \n",
      "  740 www.liberation.fr         0.4202459404368624  \n",
      "  727 www.futura-sciences.com   0.5196529997634026  \n",
      "  609 www.sudouest.fr           0.4568850933436745  \n",
      "  513 www.lexpress.fr           0.4407010383993757  \n",
      "  501 www.huffingtonpost.fr     0.5106185259951724  \n",
      "  498 www.lesechos.fr           0.46647912801407854 \n",
      "  463 www.parismatch.com        0.4395420449712281  \n",
      "  416 www.la-croix.com          0.20066893782128942 \n",
      "  336 www.jeuneafrique.com      0.43589807879973336 \n",
      "  315 www.ladepeche.fr          0.4850935107965183  \n",
      "  312 www.lepoint.fr            0.5178235479838548  \n",
      "  311 www.slate.fr              0.42808490388224946 \n",
      "  297 www.rfi.fr                0.46192844432233415 \n",
      "  280 www.capital.fr            0.43275189344074005 \n",
      "  256 www.20minutes.fr          0.48937338437936606 \n",
      "  240 www.contrepoints.org      0.37149578826822655 \n",
      "  210 www.europe1.fr            0.49304561110819944 \n",
      "  202 sante.lefigaro.fr         0.5418918925336087  \n",
      "  194 www.francetvinfo.fr       0.4946109297204234  \n",
      "  164 www.telerama.fr           0.48900725374389425 \n",
      "  144 www.letudiant.fr          0.43952639070752836 \n",
      "  130 madame.lefigaro.fr        0.49914111647858705 \n",
      "  127 www.ledauphine.com        0.447880849740394   \n",
      "  125 next.liberation.fr        0.4201649731696304  \n",
      "  117 www.lesinrocks.com        0.4258099500102808  \n",
      "  108 www.lejdd.fr              0.4223965638413931  \n",
      "  101 www.gala.fr               0.5079818172751419  \n",
      "  100 www.marieclaire.fr        0.4345252495077471  \n"
     ]
    }
   ],
   "source": [
    "most_common_dict = {}\n",
    "for domain in domain_distribution.keys():\n",
    "    for Res in result_list:\n",
    "        if Res.domain == domain:\n",
    "            if domain not in most_common_dict.keys():\n",
    "                most_common_dict[domain] = [ttr(Res.article_text)]\n",
    "            else:\n",
    "                most_common_dict[domain] += [ttr(Res.article_text)]\n",
    "\n",
    "most_common_domains = FreqDist(domain_distribution).most_common(30)    \n",
    "\n",
    "for domain, count in most_common_domains:\n",
    "    for domain2, lengths in most_common_dict.items():\n",
    "        if domain == domain2:\n",
    "            print('{:5} {:<25} {:<20}'.format(count, domain, mean(lengths)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
